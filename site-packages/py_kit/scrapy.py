# -*- coding: utf-8 -*-


from cve_ease.conf import gconfig
import requests
import os

class Scraper():
    def __init__(self):
        pass

class ScrapyCNNVD(Scraper):
    pass

class ScrapyCVE(Scraper):
    pass

class ScrapyNVD(Scraper):
    pass

class ScrapyCVRF(Scraper):
    def __init__(self):
        Scraper.__init__(self)
        self.config = gconfig['cvrf']
        self.job_num = int(self.config['job_num'])
        self.try_times = int(self.config['try_times'])
        self.cvrf_cache_path = self.config['cvrf_cache_path']
        self.cvrf_base_url = self.config['cvrf_base_url']
        self.cvrf_index_url = self.config['cvrf_index_url']

    def scrapy_cvrf_index(self):
        for try_index in range(self.try_times):
            try:
                response = requests.get(url=self.cvrf_index_url, timeout=(10, 30))
            except Exception as e:
                print("scrapy from api '%s' error!" % self.cvrf_index_url, str(e))
                if try_index == self.try_times - 1:
                    print("try [%d] times failed! exit.")
                    exit(1)
                print(" try again [%d/%d] " % (try_index + 1, self.try_times))
                continue
            break
        if response.status_code < 200 or response.status_code > 299:
            print("ret code no in [200,300)")
            exit(1)
        index_list = response.text.split('\n')
        self.index_total = len(index_list)
        if self.index_total == 0:
            raise " failed to get cvrf list"
        return index_list

    def process_per_cvrf(self, urls_with_index):

        import urllib
        from urllib.parse import unquote
        import shutil

        (index, url) = urls_with_index
        download_url = os.path.join(self.cvrf_base_url, url)
        download_url = unquote(download_url)
        file_name = os.path.basename(url)
        dir_path = os.path.join(self.cvrf_cache_path, os.path.dirname(url))
        file_path = os.path.join(self.cvrf_cache_path, file_name)


        if not os.path.exists (file_path) or os.path.getsize(file_path) == 0: 
            try:
                file_size = 0
                request = urllib.request.Request(download_url)
                request.add_header("Range", "bytes={}-".format(file_size))
                with urllib.request.urlopen(request) as response, open(file_path, "wb") as file:
                    shutil.copyfileobj(response, file)
            except Exception as e:
                print(f"({os.getpid()}) [ {index}/{self.index_total} ] failed! {download_url} {str(e).strip()}")
            else:
                print(f"({os.getpid()}) [ {index}/{self.index_total} ] Downloading {download_url}")
        else:
            print(f"({os.getpid()}) [ {index}/{self.index_total} ] {download_url} exist!")

